A computer is an electronic device that can perform calculations.

When most people hear the word “computer” they picture general multipurpose devices:
  Desktop
  Laptop
  Tablets
  Smartphone
  Smartwatch

Plenty of other devices do specific computing jobs, focusing on a single task or set of similar tasks:
  Internet of Things (IoT) thermostat
  Digital watch
  Router
  Wi-Fi picture frame
  Game Consoles
  Streaming Devices
  Point of sale (POS) system
  Cars
  Airplanes
  Boats
  Home Security alarms








A modern general computing device consists of three major layers:
  Hardware: Physical Parts
  Operating system: Controls hardware resources for Applications
  Applications: enables specialized compute tasks








At the most basic level, computers work through three stages, what’s called the computing process:
  Input
  Processing
  Output

Modern computing devices almost always have two other stages:
  Data Storage
  Network Connection








#### Connectors

Before diving into individual components, take a moment to consider how a computing systems components are connected

Components of an electrical circuit are electrically connected if an electric current can run between them through an electrical conductor.

  An electrical connector is any electromechanical device used to create an electrical connection between otherwise discrete electrical circuits, joining them into a larger circuit.

Electrical connectors are considered a physical interface and constitute part of the physical layer in the OSI model of networking.


Many connectors are keyed with some mechanical component (sometimes called a keyway), which prevents mating in an incorrect orientation.

Keying is used to prevent mechanical damage to connectors
  prevent incompatible and dangerous electrical connections
  prevents otherwise symmetrical connectors from being connected in the wrong orientation or polarity.

Most electrical connectors have a gender – i.e. the male component, called a plug, connects to the female component, or socket. The connection may be removable (as for portable equipment), require a tool for assembly and removal, or serve as a permanent electrical joint between two points. An adapter can be used to join dissimilar connectors.

Thousands of configurations of connectors are manufactured for power, data, and audiovisual applications. Electrical connectors can be divided into four basic categories, differentiated by their function:

  inline or cable connectors
    permanently attached to a cable, so it can be plugged into another terminal (either a stationary instrument or another cable)
  Chassis or panel connectors
    permanently attached to a piece of equipment so users can connect a cable to a stationary device
  PCB mount connectors
    soldered to a printed circuit board, providing a point for cable or wire attachment.
    (e.g. pin headers, screw terminals, board-to-board connectors)
  Splice or butt connectors
    that permanently join two lengths of wire or cable
      (Socket to ribbon connection on Female DE-15 used for VGA)


    In addition to the classes mentioned above, connectors are characterized by their pinout, method of connection, materials, size, contact resistance, insulation, mechanical durability, ingress protection, lifetime (number of cycles), and ease of use.

    It is usually desirable for a connector to be easy to identify visually, rapid to assemble, inexpensive, and require only simple tooling. In some cases an equipment manufacturer might choose a connector specifically because it is not compatible with those from other sources, allowing control of what may be connected. No single connector has all the ideal properties for every application; the proliferation of types is a result of the diverse yet specific requirements of manufacturers.

    Electrical connectors essentially consist of two classes of materials: conductors and insulators.
      Conductor materials must have high contact resistance, conductivity, mechanical strength, formability, and resilience.
      Insulator materials must have high electrical resistance, high temperature resistance, and be easy to manufacture for a precise fit.

    Electrodes in connectors are usually made of copper alloys, due to their good conductivity and malleability. Alternatives include brass, phosphor bronze, and beryllium copper. The base electrode metal is often coated with another inert metal such as gold, nickel, or tin. The use of a coating material with good conductivity, mechanical robustness and corrosion resistance helps to reduce the influence of passivating oxide layers and surface adsorbates, which limit metal-to-metal contact patches and contribute to contact resistance. For example, copper alloys have favorable mechanical properties for electrodes, but are hard to solder and prone to corrosion. Thus, copper pins are usually coated with gold to alleviate these pitfalls, especially for analog signals and high reliability applications.

    Contact carriers that hold the parts of a connector together are usually made of plastic, due to its insulating properties. Housings or backshells can be made of molded plastic and metal.[7]: 15  Connector bodies for high-temperature use, such as thermocouples or associated with large incandescent lamps, may be made of fired ceramic material.

    Connectors are purely passive components – that is, they do not enhance the function of a circuit – so connectors should affect the function of a circuit as little as possible. Insecure mounting of connectors (primarily chassis-mounted) can contribute significantly to the risk of failure, especially when subjected to extreme shock or vibration.[11] Other causes of failure are connectors inadequately rated for the applied current and voltage, connectors with inadequate ingress protection, and threaded backshells that are worn or damaged.

High temperatures can also cause failure in connectors, resulting in an "avalanche" of failures – ambient temperature increases, leading to a decrease in insulation resistance and increase in conductor resistance; this increase generates more heat, and the cycle repeats.

Some connector housings are designed with locking mechanisms to prevent inadvertent disconnection or poor environmental sealing.[1] Locking mechanism designs include locking levers of various sorts, jackscrews, screw-in shells, push-pull connector, and toggle or bayonet systems. Some connectors, particularly those with large numbers of contacts, require high forces to connect and disconnect. Locking levers and jackscrews and screw-in shells for such connectors frequently serve both to retain the connector when connected and to provide the force needed for connection and disconnection.






### Computer Components

Computers are composed of distinct components, and may contain:
  Motherboards
  Processors
  Memory
  Storage devices
  Power supplies
  Display devices
  Input devices
  Adapter cards
  Ports and cables
  Cooling systems

Most computers today are modular. Each component may be replaced with a
component of similar function, modifying performance.

You’ll sometimes hear people refer to the “big three” computer parts, which are the motherboard, processor, and memory. Without these, a computer won’t work, whereas if a sound card fails, it’s probably not going to make the entire system inoperable.












### Motherboard
Each internal and external PC component connects, directly or indirectly, to a single circuit board, the motherboard.

Also known as the system board or mainboard (less commonly, planar board).

A printed circuit board (PCB),
a conductive meshwork constructed from series of copper pathways, called traces are laminated to a nonconductive substrate made of fiberglass that lines the bottom of the computer and is often of a uniform color, such as green, brown, blue, black, or red.

Power, data, and control signals travel to all connected components through these pathways. A group of these wires assigned to a set of functions is collectively called a bus.


On the system board, you will find the central processing unit (CPU), underlying circuitry, expansion slots, video components, random access memory (RAM) slots, and a variety of other chips.





Motherboard Form Factors

System boards are classified by their form factor (design specifications).

Their form factor defines
  the size of the motherboard
  the type and location of its components
  compatible power supplies,  
  and the corresponding PC case that will accommodate the above


There are several popular motherboard form factors, each with different layouts, components, and specifications.



Exercise care and vigilance when acquiring a motherboard and components separately. Motherboards will have different expansion slots, support certain processors and memory, and fit into some cases but not others.

A motherboard’s form factor does not tell you what CPUs and memory it is compatible with; there is little to no relationship there.

However a given make of motherboard will use only certain CPUs and types of memory. Therefore, if you decide to build a computer from components, you must ensure that the motherboard, power supply, CPU, memory, and case will all work together.

Be sure that the other parts are physically compatible with the motherboard you choose. Helpful websites assist in this goal:
https://pcpartpicker.com/



Advanced Technology Extended
Intel developed the Advanced Technology eXtended (ATX) motherboard in 1996  to improve upon the classic AT-style motherboard architecture that had ruled the PC world for many years.


ATX has been updated many times over the years—both officially and through proprietary variations by manufacturers.

The original ATX motherboard measures approximately 12' wide by 9.6' from front to back (305 mm × 244 mm).

ATX (and its derivatives, such as micro-ATX) is the primary PC motherboard form factor in use today.

ATX                 12" × 9.6"    (305 mm × 244 mm)
Micro-ATX (mATX)    9.6" × 9.6"   (244 mm × 244 mm)
Flex-ATX            9" × 7.5"     (228.6 mm × 190.5 mm)
Mini-ATX            5.9" × 5.9"   (150 mm × 150 mm)

Ultra ATX           14.4" × 9.6"  (366 × 244 mm)
Extended ATX (EATX) 12" x 13"     (305 × 330 mm)


ITX                 8.5" × 7.5"   (215 mm × 191 mm)
Mini-ITX (mITX )    6.7" × 6.7"   (170 mm × 170 mm)
Nano-ITX            4.7" × 4.7"   (120 mm × 120 mm)
Pico-ITX            3.9" × 2.8"   (100 mm × 72 mm)
Mobile-ITX          2.9" × 1.77"  (75 mm × 45 mm)


✓■ Mobile- ITX— 2.4′′ × 2.4′′ (60 mm × 60 mm)




The ATX motherboard arrangement
  has the processor and memory slots at right angles to the expansion cards.
  puts the processor and memory in line with the fan output of the power supply, allowing the processor to run cooler.

  And because those components are not in line with the expansion cards, you can install full-length expansion cards—adapters that extend the full length of the inside of a standard computer case— in an ATX motherboard machine.




Information Technology eXtended
The Information Technology eXtended (ITX)

ITX, originally named EPIA, was developed by chipset-company VIA Technologies in 2001 as a low-power, small form factor (SFF) board for specialty uses.

Subsequent designs—the Mini-ITX, Nano-ITX, Pico-ITX, and Mobile-ITX—have been progressively smaller and targeted to embedded systems.

Including home-theater systems, compact desktop systems, gaming systems, and other embedded components.




The mini-ITX motherboard has four mounting holes that line up with three or four of the holes in the ATX and micro-ATX form factors. In mini-ITX boards, the rear interfaces are placed in the same location as those on the ATX motherboards. These features make mini-ITX boards compatible with ATX cases.

This is where the mounting compatibility ends, because despite the PC compatibility of the other ITX form factors they lack the requisite mounting and interface specifications.

Notice that the “m” in mATX stands for micro, whereas the “m” in mITX stands for mini.




### MotherBoard Components

The following list summarizes key concepts you need to
know about motherboards:
✓■ Bus architecture
✓■ Chipsets
✓■ Expansion slots
✓■ Memory slots and cache
✓■ Central processing units and processor sockets
✓■ Power connectors
✓■ Onboard non-volatile storage connectors (such as for hard drives)
✓■ Motherboard headers
✓■ BIOS/UEFI/firmware
✓■ CMOS and CMOS battery
✓■ Front-panel connectors and headers



### Bus Architecture

In a PC, data is sent from one component to another via a bus, which is a common collection of signal pathways.


#### Bus History
In the very early days, PCs used serial buses, which sent one bit at a time. Engineers first redesigned the bus to send 8 bits at a time (over synchronized separate lines), which resulted in a big speed increase. This was known as a parallel bus.
The downside of parallel communications is the loss of circuit length (how long the circuit could be) and throughput (how much data could move at one time). The signal could travel only a short distance, and the amount of data was limited due to the careful synchronization needed between separate lines, the speed of which must be controlled to limit skewing the arrival of the individual signals at the receiving end.


What was once old is new again, as engineers have discovered methods to make serial transmissions work at data rates that are many times faster than parallel signals.

Nearly everything you see today uses a serial bus. The only limitation of serial circuits is in the capability of the transceivers, which tends to grow over time at a refreshing rate due to technical advancements.

Examples of specifications that have heralded the dominance of serial communications are Serial Advanced Technology Attachment (Serial ATA, or SATA), Universal Serial Bus (USB), IEEE 1394/FireWire, and Peripheral Component Interconnect Express (PCIe).

The term bus is also used in any parallel or bit-serial wiring implementation where multiple devices can be attached at the same time in parallel or in series (daisy- chained). Examples include Small Computer System Interface (SCSI), USB, and Ethernet.


#### Motherboard Buses
On a motherboard, several different buses are used.

  Expansion slots of various architectures, such as PCIe, are included to allow for the insertion of external devices or adapters.
  Other types of buses exist within the system to allow communication between the CPU, RAM, and other components with which data must be exchanged. Except for CPU slots and sockets and memory slots, there are no insertion points for devices in many closed bus systems because no adapters exist for such an environment.

The various buses throughout a given computer system can be rated by their bus speeds. The higher the bus speed, the higher its performance.

In some cases, various buses must be synchronized for proper performance, such as the system bus and any expansion buses that run at the front-side bus speed.
Other times, one bus will reference another for its own speed.

The internal bus speed of a CPU is derived from the front-side bus clock, for instance. The buses presented throughout this chapter are accompanied by their speeds, where appropriate.



Chipsets
A chipset is a collection of chips or circuits that perform interface and peripheral functions
for the processor. This collection of chips is usually the circuitry that provides interfaces for
memory, expansion cards, and onboard peripherals, and it generally dictates how a mother-
board will communicate with the installed peripherals.
Don’t worry about memorizing any chipset names—you can look them up online to understand
their features.

Chipsets can be made up of one or several integrated circuit chips. Intel-based mother-
boards, for example, typically use two chips. To know for sure, you must check the manu-
facturer’s documentation, especially because cooling mechanisms frequently obscure today’s
chipset chips, sometimes hindering visual brand and model identification.
Chipsets can be divided into two major functional groups, called Northbridge and South-
bridge.





Northbridge


The Northbridge subset of a motherboard’s chipset is the set of circuitry or chips that per-
forms one very important function: management of high- speed peripheral communications.
The Northbridge is responsible primarily for communications with integrated video using
PCIe, for instance, and processor-to- memory communications. Therefore, it can be said that
much of the true performance of a PC relies on the specifications of the Northbridge compo-
nent and its communications capability with the peripherals it controls.

The communications between the CPU and memory occur over what is known as the
front-side bus (FSB), which is just a set of signal pathways connecting the CPU and main
memory, for instance. The clock signal that drives the FSB is used to drive communications
by certain other devices, such as PCIe slots, making them local-bus technologies. The back-
side bus (BSB), if present, is a set of signal pathways between the CPU and external cache
memory. The BSB uses the same clock signal that drives the FSB. If no back- side bus exists,
cache is placed on the front-side bus with the CPU and main memory.
The Northbridge is directly connected to the Southbridge (discussed next). It controls the
Southbridge and helps to manage the communications between the Southbridge and the rest
of the computer.


Southbridge
The Southbridge subset of the chipset is responsible for providing support to the slower
onboard peripherals (USB, Serial and Parallel ATA, parallel ports, serial ports, and so on),
managing their communications with the rest of the computer and the resources given to
them. These components do not need to keep up with the external clock of the CPU and
do not represent a bottleneck in the overall performance of the system. Any component
that would impose such a restriction on the system should eventually be developed for FSB
attachment.
In other words, if you’re considering any component other than the CPU, memory and
cache, or PCIe slots, the Southbridge is in charge. Most motherboards today have integrated
USB, network, and analog and digital audio ports for the Southbridge to manage
The Southbridge is also responsible for managing
communications with the slower expansion buses, such as PCI, and legacy buses.


#### Expansion Cards
The motherboard is the main circuit board, which contains the external data bus and connection for expansion devices that are not part of the board's basic design. The expansion slots act as "on ramps" to the external bus. Expansion cards, once commonly known as "daughter cards," are placed in slots on the motherboard. Other forms of onramp are the slots that hold memory or the sets of pins used to attach drive cables. Connectors on the motherboard grant access to the data bus for keyboards, mouse devices, and peripheral devices like modems and printers through the use of COM and LPT ports.

Expansion Slots
The most visible parts of any motherboard are the expansion slots. These are small plastic
slots, usually from 1 to 6 inches long and approximately ½ inch wide. As their name sug-
gests, these slots are used to install various devices in the computer to expand its capabilities.
Some expansion devices that might be installed in these slots include video, network, sound,
and disk interface cards.

PCI Expansion Slots
It’s now considered an old technology, but many motherboards in use today still contain 32-
bit Peripheral Component Interconnect (PCI) slots. They are easily recognizable because they
are only around 3 inches long and classically white, although modern boards take liberties
with the color. PCI slots became extremely popular with the advent of Pentium-class pro-
cessors in the mid- 1990s. Although popularity has shifted from PCI to PCIe, the PCI slot’s
service to the industry cannot be ignored; it has been an incredibly prolific architecture for
many years.
PCI expansion buses operate at 33 MHz or 66 MHz (version 2.1) over a 32-bit (4- byte)
channel, resulting in data rates of 133 MBps and 266 MBps, respectively, with 133 MBps
being the most common, server architectures excluded. PCI is a shared- bus topology, how-
ever, so mixing 33 MHz and 66 MHz adapters in a 66 MHz system will slow all adapters to
33 MHz. Older servers might have featured 64- bit PCI slots as well, since version 1.0, which
double the 32- bit data rates.

PCI slots and adapters are manufactured in 3.3V and 5V versions. Universal adapters
are keyed to fit in slots based on either of the two voltages. The notch in the card edge of
the common 5V slots and adapters is oriented toward the front of the motherboard, and
the notch in the 3.3V adapters toward the rear.

PCIe Expansion Slots
The most common expansion slot architecture that is being used by motherboards is PCI
Express (PCIe). It was designed to be a replacement for PCI, as well as an older video card
standard called accelerated graphics port (AGP). PCIe has the advantage of being faster than
AGP while maintaining the flexibility of PCI. PCIe has no plug compatibility with either AGP
or PCI. Some modern PCIe motherboards can be found with regular PCI slots for backward
compatibility, but AGP slots have not been included for many years.
PCIe is casually referred to as a bus architecture to simplify its comparison with other
bus technologies. True expansion buses share total bandwidth among all slots, each of which
taps into different points along the common bus lines. In contrast, PCIe uses a switching
component with point- to- point connections to slots, giving each component full use of the
corresponding bandwidth and producing more of a star topology versus a bus. Furthermore,
unlike other expansion buses, which have parallel architectures, PCIe is a serial technology,
striping data packets across multiple serial paths to achieve higher data rates.
PCIe uses the concept of lanes, which are the switched point- to- point signal paths bet-
ween any two PCIe components. Each lane that the switch interconnects between any two
intercommunicating devices comprises a separate pair of wires for both directions of traffic.
Each PCIe pairing between cards requires a negotiation for the highest mutually supported
number of lanes. The single lane or combined collection of lanes that the switch intercon-
nects between devices is referred to as a link.
There are seven different link widths supported by PCIe, designated x1 (pronounced
“by 1”), x2, x4, x8, x12, x16, and x32, with x1, x4, and x16 being the most common. The
x8 link width is less common than these but more common than the others. A slot that sup-
ports a particular link width is of a physical size related to that width because the width is
based on the number of lanes supported, requiring a related number of wires. As a result, an
x8 slot is longer than an x1 slot but shorter than an x16 slot. Every PCIe slot has a 22-pin
portion in common toward the rear of the motherboard, which you can see in Figure 1.7, in
which the rear of the motherboard is to the left. These 22 pins comprise mostly voltage and
ground leads.

Four major versions of PCIe are currently available in the market: 1.x, 2.x, 3.0, and 4.0.
For the four versions, a single lane, and therefore an x1 slot, operates in each direction (or
transmits and receives from either communicating device’s perspective), at a data rate of
250 MBps (almost twice the rate of the most common PCI slot), 500 MBps, approximately
1 GBps, and roughly 2 GBps, respectively.

An associated bidirectional link has a nominal throughput of double these rates. Use the
doubled rate when comparing PCIe to other expansion buses because those other rates are
for bidirectional communication. This means that the 500 MBps bidirectional link of an
x1 slot in the first version of PCIe was comparable to PCI’s best, a 64-bit slot running at
66 MHz and producing a throughput of 533 MBps.

PCIe 5.0 was formally ratified by the PCI Special Interest Group (PCI-SIG)
in 2019, and motherboards supporting the architecture started hitting the
market in late 2021. Much like its predecessors, it doubles the speed of
the previous version. Therefore, a PCIe 5.0 x1 adapter operates at about
4 MBps in each direction. The slots are forward and backward compat-
ible. For example, you can put a PCIe 4.0 video card into a motherboard
with a PCIe 5.0 slot, but you won’t get the full performance that PCIe 5.0
is capable of. The inverse is true as well—a PCIe 5.0 card will work in a 4.0
slot but at the 4.0 standard’s speed. PCIe 6.0 is expected around 2023.

Combining lanes simply results in a linear multiplication of these rates. For example,
a PCIe 1.1 x16 slot is capable of 4 GBps of throughput in each direction, 16 times the
250 MBps x1 rate. Bidirectionally, this fairly common slot produces a throughput of 8 GBps.
Each subsequent PCIe specification doubles this throughput. The aforementioned PCIe
5.0 will produce bidirectional throughput of approximately 128 GBps, which is faster than
some DDR4 standards (which is to say, it’s really, really fast).

Using Shorter Cards in Longer Slots
Up- plugging is defined in the PCIe specification as the ability to use a higher-capability slot
for a lesser adapter. In other words, you can use a shorter (fewer-lane) card in a longer slot.
For example, you can insert an x8 card into an x16 slot. The x8 card won’t completely fill
the slot, but it will work at x8 speeds if up- plugging is supported by the motherboard. Oth-
erwise, the specification requires up- plugged devices to operate at only the x1 rate. This is
something you should be aware of and investigate in advance. Down-plugging is possible
only on open- ended slots, although not specifically allowed in the official specification.
Even if you find or make (by cutting a groove in the end) an open-ended slot that accepts a
longer card edge, the inserted adapter cannot operate faster than the slot’s maximum rated
capability because the required physical wiring to the PCIe switch in the Northbridge is
not present.


Because of its high data rate, PCIe is the current choice of gaming aficionados. Addition-
ally, technologies similar to NVIDIA’s Scalable Link Interface (SLI) allow such users to com-
bine preferably identical graphics adapters in appropriately spaced PCIe x16 slots with a
hardware bridge to form a single virtual graphics adapter. The job of the bridge is to provide
non-chipset communication among the adapters. The bridge is not a requirement for SLI to
work, but performance suffers without it. SLI- ready motherboards allow two, three, or four
PCIe graphics adapters to pool their graphics processing units (GPUs) and memory to feed
graphics output to a single monitor attached to the adapter acting as the primary SLI device.
SLI implementation results in increased graphics performance over single- PCIe and non-PCIe
implementations.

Using Riser Cards
Most PC expansion cards plug directly into the motherboard. A special type of expansion
card, called a riser card, provides additional slots that other expansion cards plug into.
The other expansion cards are then parallel to the motherboard, as opposed to perpendic-
ular.
Riser cards aren’t often found in desktop PCs today but do still find some use in rack-
mounted servers with low-profile cases. The motherboard must be designed to accept a
riser card, and the case needs to be built for it as well, for the external portion of the expan-
sion card to be accessible.










Fan and heatsink
the processor is the easiest component to identify on the
motherboard. It is usually the component that has either a fan or a heat sink (usually both) attached to it

These devices are used to draw away and disperse
the heat that a processor generates.

Today’s processors generate enough heat that, without heat dispersal, they would
permanently damage themselves and the motherboard in a matter of minutes, if not seconds.


CPU Socket
Typically flat with several columns and rows of holes or pins arranged in a square.

Differing sockets



Socket AM4, made for AMD processors such as
the Ryzen, and has holes to receive the pins on the CPU. This is known as a pin grid array
(PGA) arrangement for a CPU socket. The holes and pins are in a row/column orientation,
an array of pins. The right socket is known as LGA 1200, and there are spring-loaded pins in
the socket and a grid of lands on the CPU. The land grid array (LGA) is a newer technology
that places the delicate pins (yet more sturdy than those on chips) on the cheaper mother-
board instead of on the more expensive CPU, opposite to the way that the aging PGA does.
The device with the pins has to be replaced if the pins become too damaged to function.

Modern CPU sockets have a mechanism in place that reduces the need to apply considerable force to the CPU to install a processor, which was necessary in the early days of personal computing.

Given the extra surface area on today’s processors, excessive pressure applied in the wrong manner could damage the CPU packaging, its pins, or the motherboard itself. For CPUs based on the PGA concept, zero insertion force (ZIF) sockets are exceedingly popular. ZIF sockets use a plastic or metal lever on one of the two lateral edges to lock
or release the mechanism that secures the CPU’s pins in the socket. The CPU rides on the
mobile top portion of the socket, and the socket’s contacts that mate with the CPU’s pins
are in the fixed bottom portion of the socket.


For processors based on the LGA concept, a socket with a different locking mechanism
is used. Because there are no receptacles in either the motherboard or the CPU, there is no
opportunity for a locking mechanism that holds the component with the pins in place. LGA-
compatible sockets, as they’re called despite the misnomer, have a lid that closes over the
CPU and is locked in place by an L-shaped arm that borders two of the socket’s edges. The
nonlocking leg of the arm has a bend in the middle that latches the lid closed when the other
leg of the arm is secured.

Listing out all the desktop PC socket types you might encounter would take a long
time. Instead, we’ll give you a sampling of some that you might see. The first thing you
might notice is that sockets are made for Intel or AMD processors, but not both. Keep that
compatibility in mind when replacing a motherboard or a processor. Make sure that the
processor and motherboard were designed for each other (even within the Intel or AMD
families); otherwise, they won’t fit each other and won’t work.

Servers and laptops/tablets generally have different
sockets altogether, although some CPU sockets will support processors designed for desktops
or servers.

Multisocket and Server Motherboards
When it comes to motherboard compatibility, the two biggest things to keep in mind are the
processor type and the case. If either of those are misaligned with what the motherboard
supports, you’re going to have problems.
Thus far, as we’ve talked about desktop motherboards and their CPU sockets, we have
shown examples of boards that have just one socket. There are motherboards that have
more than one CPU socket and conveniently, they are called multisocket (typically written
as two words) motherboards.


while server motherboards are often
ATX-sized, many server manufacturers create custom boards to fit inside their chassis.
Regardless, multisocket and server motherboards will generally use the same CPU sockets
that other motherboards use.


Mobile Motherboards
In small mobile devices, space is at a premium. Some manufacturers will use standard
small- factor motherboards, but are custom boards to fit inside specific cases.

Nearly all laptop processors are soldered onto the motherboard, so you don’t have to
worry about CPU socket compatibility. If the CPU dies, you replace the entire motherboard.


Power Connectors
In addition to these sockets and slots on the motherboard, a special connector (the 24-pin molex) allows the motherboard to be connected to
the power supply to receive power.

CPUs are typically a single silicon-based electronic chip
usually a thin wafer of silicon and tiny transistors
handles the majority of the processing tasks



Onboard Nonvolatile Storage Connectors

Integrated Drive Electronics/Parallel ATA
At one time, integrated drive electronics (IDE) drives were the most common type of hard drive found in computers.

  Though often thought of in relation to hard drives, IDE was much more than a hard drive interface; it was also a popular interface for many other drive types, including optical drives and tape drives.

Today, we call it IDE Parallel ATA (PATA) and consider it to be a legacy technology. shows two PATA interfaces; you can see that
one pin in the center is missing (as a key) to ensure that the cable gets attached properly.

The industry now favors Serial ATA.

Serial ATA
Serial ATA (SATA) began as an enhancement to the original ATA specifications, also known
as IDE and, today, PATA. Technology is proving that the orderly progression of data in a
single- file path is superior to placing multiple bits of data in parallel and trying to synchro-
nize their transmission to the point that all of the bits arrive simultaneously. In other words,
if you can build faster transceivers, serial transmissions are simpler to adapt to the faster
rates than are parallel transmissions.
The first version of SATA, known as SATA 1.5 Gbps (and also by the less-preferred terms
SATA I and SATA 150), used an 8b/10b-encoding scheme that requires 2 non-data overhead
bits for every 8 data bits. The result is a loss of 20 percent of the rated bandwidth. The sil-
ver lining, however, is that the math becomes quite easy. Normally, you have to divide by 8
to convert bits to bytes. With 8b/10b encoding, you divide by 10. Therefore, the 150 MBps
throughput for which this version of SATA was nicknamed is easily derived as 1/10 of the
1.5 Gbps transfer rate. The original SATA specification also provided for hot swapping at the
discretion of the motherboard and drive manufacturers.
Similar math works for SATA 3 Gbps, tagged as SATA II and SATA 300, and SATA
6 Gbps, which you might hear called SATA III or SATA 600. Note that each subsequent ver-
sion doubles the throughput of the previous version.


Note that identifiers silkscreened onto
motherboards often enumerate SATA headers. The resulting numbers are not related to the SATA version that the header supports. Instead, such numbers serve to differentiate headers from one another and to map to firmware identifiers, often visible within the BIOS configuration utility.



Another version of SATA that you will see is external SATA (eSATA). As you might
expect based upon the name, this technology was developed for devices that reside outside
of the case, not inside it. Many motherboards have an eSATA connector built in. If not, you
can buy an expansion card that has eSATA ports and plugs into internal SATA connectors

inally, SATA and eSATA
standards are compatible. In other words, SATA 6 Gbps equals eSATA 6 Gbps.

SATA and eSATA ports do not provide power, like USB does. Therefore,
when connecting SATA and eSATA drives, a separate power connection
is required in addition to the data cable.




M.2
The most recent development in expansion connections is M.2 (pronounced “M dot 2”). So far it’s primarily used for hard drives, but other types of devices, such as Wi-Fi, Bluetooth, Global Positioning System (GPS), and near-field communication (NFC) adapters are built for
M.2 as well.

M.2 is a form factor, not a bus standard. The form factor supports existing SATA, USB, and PCIe buses. This means that if you hook up a SATA device
to an M.2 slot (with the appropriate connector), the device speed will be regulated by SATA standards.





Motherboard Headers
From the time of the very first personal computer, there has been a minimum expectation as to the buttons and LEDs that should be easily accessible to the user.

These buttons and lights, as well as other external connectors, plug into the motherboard through a series of pins known as headers.

Examples of items that are connected using a header include:
✓■ Power button
✓■ Power light
✓■ Reset button
✓■ Drive activity lights
✓■ Audio jacks
✓■ USB ports

Headers for different connections are often spread throughout different locations on the motherboard—finding the right one can sometimes be a frustrating treasure hunt.

Other headers are grouped together. For example, most of the headers for the items on the front or
top panel of the case are often co-located. The purpose for the header will be printed on the
motherboard, and while that may tell you what should connect there, it often lacks detail in
how it should be connected. The motherboard manufacturer’s website is a good place to go
if you need a detailed diagram or instructions.




Power Button and Light
Users expect a power button to use to turn the computer on.
The soft power feature available through the front power button, which
is no more than a relay, allows access to multiple effects through the contact on the motherboard, based on how long the button is pressed. These effects can be changed through the BIOS or operating system. Users also expect a power light, often a green LED, to assure them that the button did its job.

Reset Button
The reset button appeared as a way to reboot the computer from a cold startup point without removing power from the components. Keeping the machine powered tends to prolong the life of the electronics affected by power cycling. Pressing the reset button also gets around software lockups because the connection to the motherboard allows the system to restart from the hardware level. One disadvantage to power cycling is that certain circuits, such as memory chips, might need time to drain their charge for the reboot to be completely successful. This is why there is always a way to turn the computer off as well.


Drive Activity Light
In the early days of personal computing, the hard disk drive’s LED had to be driven by the drive itself. Before long, the motherboard was equipped with drive headers, so adding pins to drive the drive activity light was no issue. These days, all motherboards supply this connectivity. The benefit of having one LED for all internal drives is that all the drives are represented on the front panel when only one LED is provided. The disadvantage might be
that you cannot tell which drive is currently active. This tends to be a minor concern because
you often know which drive you’ve accessed. If you haven’t intentionally accessed any drive,
it’s likely the drive that holds the operating system or virtual-memory swap file is being
accessed by the system itself. In contrast, external drives with removable media, such as
optical drives, supply their own activity light on their faceplate.


Audio Jacks
Early generations of optical drives had to have a special cable attached to the rear of the
drive, which was then attached to the sound card if audio CDs were to be heard through the
speakers attached to the sound card. Sound emanating from a CD-ROM running an appli-
cation, such as a game, did not have to take the same route and could travel through the
same path from the drive as general data. The first enhancement to this arrangement came in
the form of a front 3.5 mm jack on the drive’s faceplate that was intended for headphones
but could also have speakers connected to it. The audio that normally ran across the special
cable was rerouted to the front jack when something was plugged into it.
Many of today’s motherboards have 10-position pin headers designed to connect to stan-
dardized front- panel audio modules. Some of these modules have legacy AC’97 analog ports
on them, whereas others have high-definition (HD) audio connections. Motherboards that
accommodate both have a BIOS setting that enables you to choose which header you want
to activate, with the HD setting most often being the default.


USB Ports
So many temporarily attached devices feature USB connectivity, such as USB keys (flash
drives) and cameras, that front-panel connectivity is a must. Finding your way to the back
of the system unit for a brief connection is hardly worth the effort in some cases. For many
years, motherboards have supplied one or more 10- position headers for internal connectivity
of front-panel USB ports. Because this header size is popular for many connectors, only 9
positions tend to have pins protruding, while the 10th position acts as a key, showing up
in different spots for each connector type to discourage the connection of the wrong cable.




Firmware


Firmware is a specific class of computer software that provides the low-level control for a device's specific hardware.

Ascher Opler potentially coined the term firmware in a 1967 Datamation article, as an intermediary term between "hardware" and "software". In this article, Opler was referring to a new kind of computer program that had a different practical and psychological purpose from traditional programs from the user's perspective.

As computers began to increase in complexity, it became clear that various programs needed to first be initiated and run to provide a consistent environment necessary for running more complex programs at the user's discretion. This required programming the computer to run those programs automatically. Furthermore, as companies, universities, and marketers wanted to sell computers to laypeople with little technical knowledge, greater automation became necessary to allow a lay-user to easily run programs for practical purposes. This gave rise to a kind of software that a user would not consciously run, and it led to software that a lay user wouldn't even know about.

Originally, it meant the contents of a writable control store (a small specialized high-speed memory on the CPU), containing microcode that defined and implemented the computer's instruction set, and that could be reloaded to specialize or modify the instructions that the central processing unit (CPU) could execute. As originally used, firmware contrasted with hardware (the CPU itself) and software (normal instructions executing on a CPU). It was not composed of CPU machine instructions, but of lower-level microcode involved in the implementation of machine instructions. It existed on the boundary between hardware and software; thus the name firmware.

Over time, popular usage extended the word firmware to denote any computer program that is tightly linked to hardware, including BIOS on PCs, boot firmware on smartphones, computer peripherals, or the control systems on simple consumer electronic devices such as microwave ovens, remote controls.

Firmware for complex devices, such as the BIOS of a personal computer, enables basic functions of a device, and provides hardware abstraction services to higher-level software such as operating systems.

For less complex devices, firmware may act as the device's complete operating system, performing all control, monitoring and data manipulation functions.

Firmware is held in non-volatile read-only memory (ROM) devices such as ROM, EPROM, EEPROM, and Flash memory.

Updating firmware requires ROM integrated circuits to be physically replaced, or EPROM or flash memory to be reprogrammed through a special procedure. Some firmware memory devices are permanently installed and cannot be changed after manufacture. Common reasons for updating firmware include fixing bugs or adding features to the device.

Flashing
Flashing involves the overwriting of existing firmware or data, contained in EEPROM or flash memory module present in an electronic device, with new data. This can be done to upgrade a device or to change the provider of a service associated with the function of the device, such as changing from one mobile phone service provider to another or installing a new operating system.

If firmware is upgradable, it is often done via a program from the provider, and will often allow the old firmware to be saved before upgrading so it can be reverted to if the process fails, or if the newer version performs worse.

Free software replacements for vendor flashing tools have been developed, such as Flashrom.




BIOS/UEFI and the POST Routine
Basic Input/Output System (BIOS)

The BIOS (memory) chip, also referred to as the ROM BIOS chip,
contains the all important BIOS system software.

The BIOS  boots the system and allows the operating system to interact with hardware in the computer in lieu of requiring a more complex device driver to do so.

    The BIOS chip is easily identified: If you have a brand-name computer, this chip might have on it the name of the manufacturer and usually the word BIOS.
    For clones, the chip usually has a sticker or printing on it from one of the major BIOS manufacturers (AMI, Phoenix, Award, Winbond, and others).
    On later motherboards, the BIOS might be difficult to identify or it might even be integrated into the Southbridge, but the functionality remains regardless of how it’s implemented.

  The 1998 copyright on the label refers to the oldest code present on the chip


Unified Extensible Firmware Interface (UEFI) is the successor to the BIOS.

The extensible features of the UEFI allow for the support of a vast array of systems and platforms by allowing the UEFI access to system resources for storage of additional modules that can be added at any time.

A security feature known as Secure Boot would not be possible with the classic BIOS. It is the extensibility of the UEFI that makes such technology feasible.


At a basic level, the BIOS/UEFI controls system boot options such as the sequence of drives from which it will look for operating system boot files.

Other interface configuration options will be available too, such as
  enabling or disabling integrated ports or an integrated video card.
   disable the USB ports (popular option on corporate computers, to increase data security and decrease the risk of contracting a virus)



Central processing unit (CPU)

also called the microprocessor,

A microprocessor is a computer processor
where the data processing logic and control
is included on a single integrated circuit,
or a small number of integrated circuits.

A microprocessor contains the arithmetic, logic, and control circuitry
required to perform the functions of a computer's central processing unit.
To interpret and execute program instructions and arithmetic operations.

A CPU invariably hides on the motherboard below a heat sink and often a fan
assembly as well.

granddaddy CPU: the Intel 8088,
invented in the late 1970s, this CPU defined the idea of the modern microprocessor and contains the same basic parts used in the most advanced CPUs today.

CPUs communicate over the external data bus (EDB)
The external data bus (also known as the external bus or simply data bus) is the primary route for data in a PC. All data-handling components or optional data devices are connected to it; therefore, any information (code) placed on that bus is available to all devices connected to the computer.

early computers used eight conductors (an 8-bit data bus), which allowed for the transfer of 1 byte of information at a time. As computers evolved, the width of the external data bus increased to 16, 32, and finally to the current width of 64 conductors. The wider bus lets more data flow at the same time, just as adding more lanes to a highway allows more cars to move through a point in a given amount of time.

Registers

store internal commands and data
These commands are called the microprocessor’s machine language.

By placing machine language commands—called lines of code—onto the EDB one
at a time, you can instruct the Man in the Box to do specific tasks. All of the machine
language commands that the CPU understands make up the CPU’s instruction set.


All CPUs contain a large number of registers, but for the moment let’s concentrate
on the four most common ones: the general-purpose registers. Intel named them AX, BX,
CX, and DX.

The 8088 was the first CPU to use the four AX–DX general-purpose
registers, and they still exist in even the latest CPUs. (But they have a
lot more light bulbs!) In 32-bit processors, the registers added an E for
extended, so EAX, EBX, and so on. The 64-bit registers get an R (I don’t know
why), thus RAX, RBX, and so on

registers
are tiny storage areas on the CPU made up of microscopic semiconductor circuits that
hold charges.

Clock

CPU is a special wire called the clock wire (most diagrams label the clock wire CLK).
A charge on the CLK wire tells the CPU that another piece of information is waiting to
be processed

For the CPU to process a command placed on the EDB, a certain minimum voltage
must be applied to the CLK wire. A single charge to the CLK wire is called a clock cycle.
The CPU requires at least two clock cycles to act on a command, and usually more. A CPU
may require hundreds of clock cycles to process some commands

The maximum number of clock cycles that a CPU can handle in a given period of
time is referred to as its clock speed. The clock speed is the fastest speed at which a CPU
can operate, determined by the CPU manufacturer. The Intel 8088 processor had a
clock speed of 4.77 MHz (4.77 million cycles per second), extremely slow by modern
standards, but still a pretty big number compared to using a pencil and paper. High-end
CPUs today run at speeds in excess of 5 GHz (5 billion cycles per second).

1 hertz (1 Hz) = 1 cycle per second
using metric prefixes to increase amplitude

A CPU’s clock speed is its maximum speed, not the speed at which it must run. A CPU can run at any speed, as long as that speed does not exceed its clock speed.

The system crystal determines the speed at which a CPU and the rest of the PC operate.
The system crystal is usually a quartz oscillator, very similar to the one in a wristwatch,
soldered to the motherboard

The quartz oscillator sends out an electric pulse at a certain speed, many millions of times per second. This signal goes first to a clock chip that adjusts the pulse, usually increasing the pulse sent by the crystal by some large multiple.

(The folks who make motherboards could connect the crystal directly to the CPU’s clock wire, but then if you wanted to replace your CPU with a CPU with a different clock speed, you’d need to replace the crystal too.)

As long as the computer is turned on, the quartz oscillator, through the clock chip, repeatedly fires a charge on the CLK wire, setting the beat, if you will, for the CPU’s activities.
If the system crystal sets a beat slower than the CPU’s clock speed, the CPU will work
just fine, though at the slower speed of the system crystal. If the system crystal forces the
CPU to run faster than its clock speed, it can overheat and stop working. Before you
install a CPU into a system, you must make sure that the crystal and clock chip send out
the correct clock pulse for that particular CPU. In the old days, this required very careful
adjustments. With today’s systems, the motherboard talks to the CPU. The CPU tells the
motherboard the clock speed it needs, and the clock chip automatically adjusts for the
CPU, making this process now invisible.

Aggressive users sometimes intentionally overclock CPUs by telling
the clock chip to multiply the pulse faster than the CPU’s designed speed.
They do this to make slower (cheaper) CPUs run faster and to get more
performance in demanding programs.









Memory
Devices that in any way hold ones and zeros that the CPU accesses are known generically as memory.

Many types of devices store ones and zeros perfectly well — but computers need memory that does more than just store groups of eight ones and zeros.

The typical increasing order of capacity and distance from the processor die is L1 cache,
L2 cache, L3 cache, RAM, and HDD/SSD

This is also the typical decreasing order of speed. The following
list includes representative capacities of these memory types. The cache capacities are for
each core of the 10th generation Intel Core i7 processor. The other capacities are simply
modern examples.
✓■ L1 cache— 80 KB (32 KB for instructions and 48 KB for data)
✓■ L2 cache— 512 KB
✓■ L3 cache— 8– 16 MB
✓■ RAM— 16–256 GB
✓■ HDD/SSD—100s of GB to several TB
One way to find out how much cache your system has is to use a utility such as CPU- Z

CPU-Z is freeware that can show you the amount of cache, processor name and number, motherboard and chipset, and memory specifications.

The CPU needs to be able to read and write to this storage medium. Additionally, this system must enable the CPU to jump to any line of stored code as easily as to any other line of code. All of this must be
done at or at least near the clock speed of the CPU. Fortunately, this magical device has existed for many years: random-access memory (RAM).

How does data get to the external data bus.
data is stored on the hard drive. In theory, you could build a computer
that sends data from the hard drive directly to the CPU, but there’s a problem — the hard drive is too slow.

Memory, or random access memory (RAM), slots are notable on a motherboard. These slots are designed for the modules that hold memory chips that make up primary memory, which is used to store currently used data and instructions for the CPU.

In PCs, RAM transfers and stores
data to and from the CPU in byte-sized chunks. RAM is therefore arranged in byte-sized
rows. Here are the terms used to talk about quantities of bits:
• Any individual 1 or 0 = a bit
• 4 bits = a nibble
• 8 bits = a byte
• 16 bits = a word
• 32 bits = a double word
• 64 bits = a paragraph or quad word

The number of bytes of RAM varies from PC to PC. In earlier PCs, from around
1980 to 1990, the typical system would have only a few hundred thousand bytes of
RAM. Today’s systems often have billions of bytes of RAM.
RAM is made of groups of semiconductor chips soldered onto small cards that
snap into your computer

The CPU accesses any one row of RAM as easily and as fast as any other row, which
explains the “random access” part of RAM. Not only is RAM randomly accessible, it’s
also fast. By storing programs on RAM, the CPU can access and run them very quickly.
RAM also stores any data that the CPU actively uses.
Computers use dynamic RAM (DRAM) for the main system memory. DRAM needs
both a constant electrical charge and a periodic refresh of the circuits; otherwise, it loses
data—that’s what makes it dynamic rather than static in content. The refresh can cause
some delays, because the CPU has to wait for the refresh to happen, but modern CPU
manufacturers have clever ways to get by this issue,

Don’t confuse RAM with mass storage devices such as hard drives and flash drives.
You use hard drives and flash drives to store programs and data permanently




For the most part, PCs today use memory chips arranged on a small circuit board. A dual
in- line memory module (DIMM) is one type of circuit board. Today’s DIMMs differ in the
number of conductors, or pins, that each particular physical form factor uses. Some common
examples include 168-, 184-, 240-, and 288-pin configurations. In addition, laptop memory
comes in smaller form factors known as small outline DIMMs (SODIMMs) and Micro-
DIMMs.

Consult the
motherboard’s documentation to determine the specific modules allowed as well as their
required orientation. The number of memory slots varies from motherboard to motherboard,
but the structure of the different slots is similar. Metal pins in the bottom make contact with
the metallic pins on each memory module. Small metal or plastic tabs on each side of the slot
keep the memory module securely in its slot.

Paging
Sometimes, the amount of primary memory installed is inadequate to service additional requests for memory resources from newly launched applications. When this condition occurs, the user may receive an “out of memory” error message and an application may fail
to launch. One solution for this is to use the hard drive as additional RAM. This space on
the hard drive is known as a swap file or a paging file. The technology in general is known as
virtual memory or virtual RAM. The paging file is called PAGEFILE.SYS in modern Micro-
soft operating systems. It is an optimized space that can deliver information to RAM at
the request of the memory controller faster than if it came from the general storage pool of the
drive. It’s located at c:\pagefile.sys by default. Note that virtual memory cannot be
used directly from the hard drive; it must be paged into RAM as the oldest contents of RAM
are paged out to the hard drive to make room. The memory controller, by the way, is the
chip that manages access to RAM as well as adapters that have had a few hardware memory
addresses reserved for their communication with the processor.
Nevertheless, relying too much on virtual memory (check your page fault statistics in the
Reliability and Performance Monitor) results in the entire system slowing down noticeably.
An inexpensive and highly effective solution is to add physical memory to the system, thus
reducing its reliance on virtual memory.








how do you
connect the RAM to the EDB in such a way that the CPU can see any one given row but
still give the CPU the capability to look at any row in RAM?
We need some type of chip between the RAM and the CPU to make the connec-
tion. The CPU needs to be able to say which row of RAM it wants, and the chip should
handle the mechanics of retrieving that row of data from the RAM and putting it on
the EDB. This chip comes with many names, but for right now just call it the memory
controller chip (MCC).

The MCC contains special circuitry so it can grab the contents of any line of RAM
and place that data or command on the EDB.

Once the MCC is in place to grab any discrete byte of RAM, the CPU needs to be
able to tell the MCC which line of code it needs. The CPU therefore gains a second set
Once the MCC is in place to grab any discrete byte of RAM, the CPU needs to be
able to tell the MCC which line of code it needs. The CPU therefore gains a second set


By turning the address bus wires on and off in different patterns, the CPU tells the
MCC which line of RAM it wants at any given moment.
The
8088 had 20 wires in its address bus
Every different pattern of ones
and zeros on these 20 wires points to one byte of RAM.

If you have 20 wires, you
would have 2^20 (or 1,048,576) combinations. Because each pattern points to one line of
code and each line of RAM is one byte, if you know the number of wires in the CPU’s
address bus, you know the maximum amount of RAM that a particular CPU can handle.
Because the 8088 had a 20-wire address bus, the 8088, therefore, had an address space of 1,048,576 bytes.


Certain
powers of 2 have names used a lot in computing. The following list explains.
1 kilo = 210 = 1024 (abbreviated as “K”)
1 kilobyte = 1024 bytes (abbreviated as “KB”)
1 mega = 220 = 1,048,576 (abbreviated as “M”)
1 megabyte = 1,048,576 bytes (abbreviated as “MB”)
1 giga = 230 = 1,073,741,824 (abbreviated as “G”)
1 gigabyte = 1,073,741,824 bytes (abbreviated as “GB”)
1 tera = 240 = 1,099,511,627,776 (abbreviated as “T”)
1 terabyte = 1,099,511,627,776 bytes (abbreviated as “TB”)

In the early days of computing there arose a need to talk about large values, but
the words hadn’t been invented. In one case, the memory address folks were trying to
describe permutations. They used values based on powers of 2 as just described. No one
had ever invented terms for 1024 or 1,048,576, so they used kilo and mega, as 1000 was
close enough to 1024 and 1,000,000 was close enough to 1,048,576.
In the meantime, computer people measuring quantities such as CPU speeds and hard
drive capacities didn’t count with powers of 2. They just needed regular 1000 for kilo and
1,000,000 for mega.
From the early 1980s until around 1990, nobody cared about this weird thing where
one word could mean two values. Everything was fine until the math nerds and the
attorneys started making trouble. To fix this, in 1998 the International Electrotechnical
Committee (IEC) invented special prefixes for binary values I call the ibis (pronounced
eee-bees).
1 kibi = 210 = 1024 (abbreviated as “Ki”)
1 mebi = 220 = 1,048,576 (abbreviated as “Mi”)
1 gibi = 230 = 1,073,741,824 (abbreviated as “Gi”)
1 tebi = 240 = 1,099,511,627,776 (abbreviated as “Ti”)

Bits and bytes are abbreviated differently. Bits get a lowercase b,
whereas bytes get a capital B. So for example, 4 Kb is 4 kilobits, but 4 KB
is 4 kilobytes. The big-B little-b standard applies all the way up the food
chain, so 2 Mb = 2 megabits; 2 MB = 2 megabytes; 4 Gb = 4 gigabits;
4 GB = 4 gigabytes; and so on.

to follow this revised naming convention, you should say, “the 8088 processor could
address one mebibyte (MiB) of memory.” The problem is that no one but math nerds
uses these ibis. If you buy RAM, the manufacturers use the term gigabyte even though
technically they should use gibibyte.

The
CPU identifies the first byte of RAM on the address bus with 00000000000000000000.
The CPU identifies the last RAM row with 11111111111111111111. When the CPU
turns off all the address bus wires, it wants the first line of RAM; when it turns on all the
wires, it wants the 1,048,576th line of RAM. O





In a basic sense, to produce a CPU requires three processes. First, a developer creates
the industry standard architecture (ISA) for the CPU. That’s the instruction set, essen-
tially how the CPU will handle code and interact with other components

You’ll see the ISA described as the CPU platform
Second, a developer designs the chip floorplan, how all the transistors and other
physical parts of the CPU interconnect. Third, a fabrication company puts all the
designs into action and creates the physical CPU according to all the architecture and
design specifications.
CPUs fit into a CPU socket on a motherboard. There are many socket
types, and a CPU only fits into a specific socket.


Three companies create the vast majority of CPUs today and all three approach the
three processes differently. Intel Corporation makes x86-64 architecture processors and,
for the most part, designs the chip and handles the fabrication as well. Advanced Micro
Devices, Inc. (AMD) also makes x86-64 architecture processors and designs the chips.
AMD is (these days) a fabless semiconductor company, meaning it relies on another
company to produce the physical processors. Finally, Arm Ltd. makes Advanced RISC
Machine (ARM) architecture processors. Arm licenses its processors to many other com-
panies, such as Apple, Samsung, and Qualcomm, who design the chips. These companies
in turn use fabrication companies to make the physical processors.

Microsoft Windows, many versions of Linux, and some versions of Apple macOS
run on the x86-64 architecture, meaning their developers wrote the operating systems
to use the x86-64 instruction set. Some versions of Google Chrome OS also run on the
x86-64 architecture.

Current versions of Windows, macOS, Linux, and Chrome OS and every version of
modern mobile devices—Apple iOS and iPadOS and Google Android—run on ARM,
which makes the ARM instruction set indisputably the most used CPU platform in the
world by far.

Every computer requires hardware designed around a specific CPU platform. An
Intel-based system, for example, requires a motherboard designed for a specific set
of Intel processors. An AMD-based system likewise requires an AMD-based mother-
board. ARM systems differ somewhat because most of them have the ARM processor
integrated into the motherboard along with other components, what’s called a system
on a chip (SoC).



Model Names
CPU makers differentiate product lines by using different product names, and these
names have changed over the years. For a long time, Intel used Pentium for its flag-
ship model, just adding model numbers to show successive generations—Pentium,
Pentium II, Pentium III, and so on. AMD used the Athlon brand in a similar fashion.
Intel uses the Core brand name for the most part these days; AMD goes with Ryzen

Most discussions on PC CPUs focus on four end-product lines: desktop PC, budget
PC, portable/mobile PC, and server computers.


Microarchitecture

CPU makers continually develop faster, smarter, and generally more capable CPUs. In
general, each company comes up with a major new design, called a microarchitecture,
every few years.

Market              Intel                                   AMD
Enthusiast          Core i7/i9                        Ryzen 9, Threadripper
Mainstream desktop  Core i7/i5/i3                     Ryzen 5, Ryzen 7
Budget desktop      Pentium, Celeron                  Ryzen 5
Portable/mobile     Mobile i7/i5/i3, Mobile Celeron   Ryzen 9/7/5/3
Server              Xeon                              EPYC
Workstation         Xeon                              Ryzen PRO, Ryzen Threadripper

 They try to minimize the number of model names in use, however, most likely for marketing purposes. This means that they release CPUs labeled as the same
model, but the CPUs inside can be very different from earlier versions of that model.

CPU companies use code names to keep track of different variations within models (see
Figure 3-20). As a tech, you need to know both the models and code names to be able
to make proper recommendations for your clients.



Desktop Versus Mobile
Mobile devices, such as portable computers, have needs that differ from those of desktop
computers, notably the need to consume as little electricity as possible.
helps extending battery charge and creating less heat.
Both Intel and AMD have engineers devoted to making excellent mobile versions of
their CPUs that sport advanced energy-saving features (see Figure 3-21). These mobile
CPUs consume much less power than their desktop counterparts. They also run in very
low power mode and scale up automatically if the user demands more power from the
CPU.
Saving energy
by making the CPU run more slowly when demand is light is generically called throttling.

ecause
most portable and mobile computing devices are very compact, they can’t dissipate heat
as quickly as a well-cooled desktop system. Mobile CPUs can scale up to handle demand-
ing tasks, but they’ll start accumulating heat quickly. As this heat nears levels that could
damage the CPU, it will engage in thermal throttling to protect itself. A system trying to
do demanding work with only a fraction of its full power available may grind to a halt!


The industry describes how much heat a busy CPU generates with
a figure (measured in watts) called its thermal design power (TDP). The TDP
can give you a rough idea of how much energy a CPU draws and what kind
of cooling it will need. It can also help you select more efficient CPUs.
TDP has been trending down over time (especially in recent years), but
it may help to have a sense of what these values look like in the real world.
The CPUs in a smartphone or tablet typically have a TDP from 2 to 15 watts,
laptop CPUs range from 7 to 65 watts, and desktop CPUs tend to range from
50 to 140 watts.

Many of the technologies developed for mobile processors migrate back into their
more power-hungry desktop siblings. That’s a bonus for the planet (and maybe your
power bill).
ARM processors took the opposite road from Intel and AMD, blossoming as mobile-
only CPUs for many years and then gradually making the transition to desktop and
server systems. With their incredibly efficient energy, processing power, and heat ratios,
ARM is rapidly cutting into the desktop and server landscape.



### Technology

Engineers have altered, enhanced, and improved CPUs in a number of ways:
• Clock multipliers
• 64-bit processing
• Virtualization support
• Parallel execution
• Multicore processing
• Integrated memory controller (IMC)
• Graphics processing unit (GPU)
• Security

#### Clock Multipliers
All modern CPUs run at some multiple of the system clock speed.

The system bus on some example machine may operate at 100 MHz.
If the clock multiplier could go up to ×32 at full load it would support a 3.2 GHz maximum speed.

Originally, CPUs ran at the speed of the bus, but engineers early on realized the CPU was the only thing doing any work much of the time. Speed up just the internal operations of the CPU and not anything else, speeds up the whole computing process.

a free program called CPU-Z may be used to display CPU details like the Bus Speed, Clock Multiplier, and resulting Core Speed

Today’s CPUs report to the motherboard through a function called CPUID (CPU
identifier). The motherboard sets the speed and multiplier automatically. You can manually override this automatic setup on many motherboards.



#### 64-Bit Processing

The EDB gradually increased in size, from 8 to 16 to 32 to 64 bits
wide. The address bus similarly jumped, going from 20 to 24 to 32 bits wide (where it
stayed for a decade).

Modern consumer CPUs support 64-bit processing, thus run compatible 64-bit operating system, such as Windows 10 or Windows 11, and 64-bit applications.

They also support 32-bit processing for 32-bit operating systems, such as some Linux distributions, and 32-bit applications.

The general-purpose registers on the CPU also move up to 64-bit.

The primary benefit of 64-bit computing is support for more that the 4 GB of memory supported with 32-bit processing.
With a 64-bit address bus, CPUs can address 2^64 bytes of memory, or 16  exabyte (2^60), abbreviated EB.

A 64-bit address bus can address 16 EB of RAM.

In practical terms, increased RAM from 64-bit computing greatly enhances the performance of programs that work with large files, such as video editing applications.


##### x86
CPUs from the early days can be lumped together as x86 CPUs, because they used
an instruction set that built upon the earliest Intel CPU architecture, a time during which leading processors ended with 86

##### x64
When the 64-bit CPUs went mainstream, marketing decided to mark applications, operating systems, and so on x64 such that consumers could quickly determine compatibility.

##### x86 - x64
It’s common to marry the two terms and describe current 64-bit CPUs as x86-64 processors.

#### Virtualization support

Intel and AMD have built in support for running more than one operating system at a
time, a process called virtualization support.

The key issue
from a CPU standpoint is that virtualization used to work entirely through software.
Programmers had to write a ton of code to enable a CPU—which was designed to run
one OS at a time—to run more than one OS at the same time.
With hardware-based
virtualization support, CPUs took a lot of the burden off the programmers

#### Parallel Execution
Modern CPUs can process multiple commands and parts of commands in parallel,
known as parallel execution. Early processors had to do everything in a strict, linear fash-
ion. The CPUs accomplish this parallelism through multiple pipelines, dedicated cache,
and the capability to work with multiple threads or programs at one time.

##### Pipelining
To get a command from the data bus, do the calculation, and then send the
answer back out onto the data bus, a CPU takes at least four steps (each of these steps is
called a stage):
          1. Fetch Get the data from the EDB.
          2. Decode Figure out what type of command needs to be executed.
          3. Execute Perform the calculation.
          4. Write Send the data back onto the EDB.

Smart, discrete circuits inside the CPU handle each of these stages.
In early CPUs,
when a command was placed on the data bus, each stage did its job and the CPU handed
back the answer before starting the next command, requiring at least four clock cycles
to process a command. In every clock cycle, three of the four circuits sat idle. Today, the
circuits are organized in a conveyer-belt fashion called a pipeline. With pipelining, each
stage does its job with each clock-cycle pulse, creating a much more efficient process.

Pipelines keep every stage of the processor busy on every click of the clock, making a
CPU run more efficiently without increasing the clock speed. Note that at this point, the
CPU has four stages: fetch, decode, execute, and write—a four-stage pipeline. No CPU
ever made has fewer than four stages, but advancements in caching (see “Cache,” next)
have increased the number of stages over the years. Current CPU pipelines contain many
more stages, up to 20 in some cases.

Pipelining isn’t perfect. Sometimes a stage hits a complex command that requires
more than one clock cycle, forcing the pipeline to stop. Your CPU tries to avoid these
stops, or pipeline stalls.

The decode stage tends to cause the most pipeline stalls; certain
commands are complex and therefore harder to decode than other commands.


The inside of the CPU is composed of multiple chunks of circuitry to handle the many types of calculations your PC needs to do.

The arithmetic logic unit (ALU) (or integer unit), handles integer math.
The typical CPU spends most of its work doing integer math.

The floating point unit (FPU) handles complex
numbers.

With a single pipeline, only the ALU or the FPU worked at any execution stage. Worse yet, floating point calculation often took many, many clock cycles to execute, forcing the CPU to stall the pipeline until the FPU
finished executing the complex command
Modern CPUs offer multiple pipelines to keep the processing going

##### Cache

Another type of memory common in PCs is cache memory, which is small and fast and logically sits between the CPU and RAM.

Cache is a very fast form of memory forged from static RAM


When you send a command to the CPU, you run lots of little programs all at
the same time. Each of these programs breaks down into some number of little pieces, called threads, and data. Each thread is a series of instructions designed to do a particular job with the data.

Modern CPUs don’t execute instructions sequentially

Pipelining CPUs work fantastically well as long as the pipelines stay filled with instructions. Because the CPU runs faster than the RAM can supply it with code, you’ll always get pipeline stalls — called wait states — because the RAM can’t keep up with the CPU. To reduce wait states, CPUs come with built-in, very high-speed RAM called static RAM (SRAM).

SRAM preloads as many instructions as possible and keeps copies of already run instructions and data in case the CPU needs to work on them again.
SRAM used in this fashion is called a cache.

Cache improves system performance by predicting what the CPU will ask for next and prefetching this information before being asked. This paradigm allows the cache to be smaller in size than the RAM itself. Only the most recently used data and code or that which is expected to be used next is stored in cache.


Intel Cache Size (Windows)
To find the total size of the L1, L2, or L3 cache for Intel® Processor, follow the steps below:

   Install the Intel® Processor Identification Utility.
   Launch the utility by typing in the search window Window Intel® Processor Identification Utility.
   Click CPU DATA.

The sizes of the caches are listed in the tool.

For L1 size follow the steps below:

   Add L1 Data Cache size and L1 instruction Cache size to get the L1 cache size per core.
   Total size of the L1 cache for all cores equals to the number of cores multiplied by the L1 cache size per core.

Example:

L1 Data cache = 32 KB per core

L1 Instruction cache = 32 KB per core

So the L1 cache size per core = 32 KB + 32 KB, which = 64 KB

There are 4 cores reported, then the total size of L1 cache = 4 X 64 KB = 256 KB.


The SRAM cache inside the early CPUs was tiny, only about 16 KB, but it improved performance tremendously.

In fact, many motherboards added an intermediary cache between CPU and RAM. These caches were much larger, usually around 128 to 512 KB. When the CPU looked for a line of code, it first went to the built-in cache; if the code wasn’t there, the CPU went to the cache on the motherboard.
The cache on the CPU was called the L1 cache because it was the one the CPU first tried. The cache on the motherboard was called the L2 cache, not because it was on the
motherboard, but because it was the second cache the CPU checked.

Eventually, engineers took this cache concept even further and added the L2 cache
onto the CPU package, then eventually right onto the CPU chip. Many modern CPUs
include three caches: an L1, an L2, and an L3 cache (Intel calls it
Smart Cache).



You’ll see three different cache designations:
Level 1 Cache L1 cache is the smallest and fastest, and it’s on the processor die itself.
In other words, it’s an integrated part of the manufacturing pattern that’s used to stamp
the processor pathways into the silicon chip. You can’t get any closer to the processor
than that.
Though the definition of L1 cache has not changed much over the years, the same is not
true for other cache levels. L2 and L3 cache used to be on the motherboard but now
have moved on- die in most processors as well. The biggest differences are the speed and
whether they are shared.
Level 2 Cache L2 cache is larger but a little slower than L1 cache. For processors with
multiple cores, each core will generally have its own dedicated L1 and L2 caches. A few
processors share a common L2 cache among the cores.
Level 3 Cache L3 cache is larger and slower than L1 or L2, and is usually shared
among all processor cores.








The L2 cache on the early CPUs that had L2 cache included on the CPU package ran
at a slower clock speed than the L1 cache. The L1 cache was in the CPU and thus ran at
the speed of the CPU. The L2 cache connected to the CPU via a tiny set of wires on the
CPU package. The first L2 caches ran at half the speed of the CPU.
The inclusion of the L2 cache on the chip gave rise to some new terms to describe the
connections between the CPU, MCC, RAM, and L2 cache. The address bus and external
data bus (connecting the CPU, MCC, and RAM) were lumped into a single term called
the frontside bus, and the connection between the CPU and the L2 cache became known

#### Multicore processing
#### Integrated memory controller (IMC)
#### Graphics processing unit (GPU)
#### Security







### Ports
system unit connection points are called ports
ports allow the connection of I/O devices
Most (such as the universal serial bus, or USB) handle both input and output.






















Software


the CompTIA A+ 1102 exam covers popular workstation operating systems:
  Microsoft Windows (versions 10 and 11 only),
  Apple macOS,
  Linux,
  Google Chrome OS


the CompTIA A+ 1102 exam covers popular smartphone/tablet operating systems:
  Google Android,
  Apple iOS,
  Apple iPadOS.


Common Operating System Functions

All OSs are not created equal, but every OS provides certain functions. Here’s a list:

• The OS communicates, or provides a method for other programs to communicate, with the hardware of the PC or device.

    Operating systems run on specific hardware. For example, if you have a 32-bit CPU, you need to install a 32-bit version of an operating system. With a 64-bit CPU, you need a 64-bit OS.

• The OS manages any user interface (UI) — a visual representation of the computer on the monitor that makes sense to the people using the computer.

• The OS enables users to determine the available installed programs and run, use, and shut down the programs of their choice.

• The OS enables users to add, move, and delete the installed programs and data.

• The OS provides a method to secure a system from all sorts of threats, such as data loss or improper access.



https://en.wikipedia.org/wiki/Operating_system#Components

OS Components

    4.1 Kernel
        4.1.1 Program execution
        4.1.2 Interrupts
            4.1.2.1 Software interrupt
            4.1.2.2 Signal
            4.1.2.3 Hardware interrupt
        4.1.3 Input/Output
            4.1.3.1 Interrupt-driven I/O
            4.1.3.2 Direct Memory Access
        4.1.4 Modes
        4.1.5 Memory management
        4.1.6 Virtual memory
        4.1.7 Multitasking
        4.1.8 Disk access and file systems
        4.1.9 Device drivers
    4.2 Networking
    4.3 Security
    4.4 User interface
        4.4.1 Graphical user interfaces


All operating systems enable you to use programs
software formats vary so widely that you can’t just install any program on any OS

Programmers do extra work to build
separate versions of a program that can run on more than one OS, called portability

compatibility concerns between OSs
A users software can restrict the list of acceptable OS choices,
likewise OS choice limits available software.
likewise with hardware
One OS may need no extra software to work with a device,
while another OS might need a special program installed to control it. Likewise, brand-
new hardware may not work well on any OS until the OS receives updates to support
the new hardware.
All of the above affects how well users on differing operating systems can
collaborate!



UI

Operating system desktop styles/user interfaces

A graphical user interface allows users to engage with
the mouse or other pointing device and click on elements.

The background is called the Desktop.

common apps are
File Browser
App Store

The context menus offer options specific to the element they are accessed from (typically via right-click)
A file, for example, gives you a context menu that differs from that of an app.


Unlike Windows or macOS, different Linux distros offer a variety of user
interfaces, called desktop environments (DEs).

Command-Line Interface

Long before GUIs with pointers and icons, operating systems used a command-line interface (CLI). Every operating system still has at least one, and for good reason: the CLI works when the GUI just can’t do the job.

On Windows, the default CLI is called PowerShell.



File Structures

Almost every operating system stores files in folders in a tree pattern.
